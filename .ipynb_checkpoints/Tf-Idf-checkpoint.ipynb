{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b093dafa658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mPOS_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# adds POS tag to each token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mlematized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lematized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOS_tag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Removes duplicate gramatical counterparts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[0mPOS_lematized_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlematized_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#adds POS tags to the lematized text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8b093dafa658>\u001b[0m in \u001b[0;36mget_lematized_text\u001b[1;34m(tags)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_lematized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0madjective_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'JJ'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'JJR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'JJS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mlemmatized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import string\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def get_processed_text(lematized_text,stop_words):\n",
    "    processed_text = []\n",
    "    for word in lematized_text:\n",
    "        if word not in stop_words:\n",
    "            processed_text.append(word)\n",
    "    return processed_text \n",
    "\n",
    "def get_stopwords(POS_lematized_tag):\n",
    "    stopwords = []\n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "    for word in POS_lematized_tag:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "    punctuations = list(str(string.punctuation))\n",
    "    stopwords = stopwords + punctuations\n",
    "\n",
    "    enc = 'utf-8'\n",
    "    with open('stopword_file.csv', 'r', encoding = enc) as f:\n",
    "        reader = csv.reader(f)\n",
    "        keywords = list(reader)\n",
    "    english_stops = [i[0] for i in keywords]\n",
    "    \n",
    "    stopwords = stopwords + punctuations\n",
    "    \n",
    "    return list(set(stopwords))\n",
    "\n",
    "def get_lematized_text(tags):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    adjective_tags = ['JJ','JJR','JJS']\n",
    "    lemmatized_text = []\n",
    "    for word in tags:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "    return lemmatized_text\n",
    "\n",
    "def get_text_from_file(filename):\n",
    "    fp = open(filename, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data\n",
    "\n",
    "def get_text(directory):\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        text += get_text_from_file(\"KeywordDocs/\" + filename)\n",
    "        break\n",
    "    return text\n",
    "\n",
    "def tf(word, blob):\n",
    "    return (float)(blob.words.count(word)) / (float)(len(blob.words))\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist): return math.log(len(bloblist) / (float)(1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "\n",
    "def create_tfIdfList(document):\n",
    "    totalLength, splitLength = len(document), int(len(document)/4000)\n",
    "    bloblist = [ document[i:i+splitLength] for i in range(0, totalLength, splitLength) ]\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        blob = tb(blob)\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        enc = 'utf-8'\n",
    "        if not os.path.isfile('Data/tf-idf.csv'):\n",
    "            with open('Data/tf-idf.csv', 'w', encoding = enc) as f:\n",
    "                columnTitleRow = \"Word, Score\\n\"\n",
    "                f.write(columnTitleRow)\n",
    "                for word, score in sorted_words:\n",
    "                    if(score >= 0.1):\n",
    "                        score = \"{},{}\\n\".format(word, round(score, 5))\n",
    "                        f.write(score)\n",
    "        else:\n",
    "            with open('Data/tf-idf.csv', 'a+', encoding = enc) as f:\n",
    "                for word, score in sorted_words: \n",
    "                    if(score >= 0.1):\n",
    "                        score = \"{},{}\\n\".format(word, round(score, 5))\n",
    "                        f.write(score)\n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    text = get_text(\"KeywordDocs\")  #gets text of all docs in string format\n",
    "    tokens = nltk.word_tokenize(text) #tokenizes the string\n",
    "    POS_tag = nltk.pos_tag(tokens) # adds POS tag to each token\n",
    "    \n",
    "    lematized_text = get_lematized_text(POS_tag) #Removes duplicate gramatical counterparts\n",
    "    POS_lematized_tag = nltk.pos_tag(lematized_text) #adds POS tags to the lematized text\n",
    "    \n",
    "    stop_words = get_stopwords(POS_lematized_tag)\n",
    "\n",
    "    processed_text = get_processed_text(lematized_text,stop_words) # gets the final set of unique words with stopwords removed\n",
    "    unique_text = list(set(processed_text))\n",
    "    create_tfIdfList(unique_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
