{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import string\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "english_stops = []\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation) and\n",
    "               \t(token.lower() not in english_stops) and len(token) > 2]\n",
    "\n",
    "def get_stopwords():\n",
    "    enc = 'utf-8'\n",
    "    with open('stopword_file.csv', 'r', encoding = enc) as f:\n",
    "        reader = csv.reader(f)\n",
    "        keywords = list(reader)\n",
    "    english_stops = [i[0] for i in keywords]\n",
    "    #print ( english_stops)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "def get_cleanTokens(directory):\n",
    "    cleanTokens = []\n",
    "    wordcount = {} \n",
    "    for filename in os.listdir(directory):\n",
    "        text = get_text(\"KeywordDocs/\" + filename)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        cleanTokens.extend(clean_tokens(tokens))\n",
    "    return cleanTokens\n",
    "\n",
    "def get_text(filename):\n",
    "    fp = open(filename, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data\n",
    "\n",
    "def tf(word, blob):\n",
    "    return (float)(blob.words.count(word)) / (float)(len(blob.words))\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist): return math.log(len(bloblist) / (float)(1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "\n",
    "def create_tfIdfList(document):\n",
    "    totalLength, splitLength = len(document), int(len(document)/4000)\n",
    "    bloblist = [ document[i:i+splitLength] for i in range(0, totalLength, splitLength) ]\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        blob = tb(blob)\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        enc = 'utf-8'\n",
    "        if not os.path.isfile('Data/tf-idf.csv'):\n",
    "            with open('Data/tf-idf.csv', 'w', encoding = enc) as f:\n",
    "                columnTitleRow = \"Word, Score\\n\"\n",
    "                f.write(columnTitleRow)\n",
    "                for word, score in sorted_words: \n",
    "                    score = \"{},{}\\n\".format(word, round(score, 5))\n",
    "                    f.write(score)\n",
    "        else:\n",
    "            with open('Data/tf-idf.csv', 'a+', encoding = enc) as f:\n",
    "                for word, score in sorted_words: \n",
    "                    score = \"{},{}\\n\".format(word, round(score, 5))\n",
    "                    f.write(score)\n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    get_stopwords()\n",
    "    clean_tokens = get_cleanTokens(\"KeywordDocs\")\n",
    "    clean_doc = ' '.join(clean_tokens)\n",
    "    create_tfIdfList(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
