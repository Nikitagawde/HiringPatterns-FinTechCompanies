{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4134\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import string\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def get_score(processed_text):\n",
    "    unique_words = list(set(processed_text))\n",
    "    text_length = len(unique_words)\n",
    "    weighted_edge = np.zeros((text_length,text_length),dtype=np.float32)\n",
    "    \n",
    "    window_size = 3\n",
    "    covered_coocurrences = []\n",
    "    score = np.zeros((text_length),dtype=np.float32)\n",
    "    \n",
    "    for i in range(0,text_length):\n",
    "        score[i]=1\n",
    "        for j in range(0,text_length):\n",
    "            if j==i:\n",
    "                weighted_edge[i][j]=0\n",
    "            else:\n",
    "                for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                    window_end = window_start+window_size\n",
    "                    window = processed_text[window_start:window_end]\n",
    "                \n",
    "                    if (unique_words[i] in window) and (unique_words[j] in window):\n",
    "                    \n",
    "                        index_of_i = window_start + window.index(unique_words[i])\n",
    "                        index_of_j = window_start + window.index(unique_words[j])\n",
    "                    \n",
    "                        # index_of_x is the absolute position of the xth term in the window \n",
    "                        # (counting from 0) \n",
    "                        # in the processed_text\n",
    "                      \n",
    "                        if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                            weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                            covered_coocurrences.append([index_of_i,index_of_j])\n",
    "    \n",
    "    connections_sum = np.zeros((text_length),dtype=np.float32)\n",
    "    for i in range(0,text_length):\n",
    "        for j in range(0,text_length):\n",
    "            connections_sum[i]+=weighted_edge[i][j]\n",
    "    \n",
    "    MAX_ITERATIONS = 50\n",
    "    d=0.85\n",
    "    threshold = 0.0001 #convergence threshold\n",
    "    \n",
    "    for iter in range(0,MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "        \n",
    "        for i in range(0,text_length):\n",
    "            summation = 0\n",
    "            \n",
    "            for j in range(0,text_length):\n",
    "                if weighted_edge[i][j] != 0:\n",
    "                    summation += (weighted_edge[i][j]/connections_sum[j])*score[j]\n",
    "            \n",
    "            score[i] = (1-d) + d*(summation)\n",
    "        \n",
    "        if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "            break\n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_processed_text(lematized_text,stop_words):\n",
    "    processed_text = []\n",
    "    for word in lematized_text:\n",
    "        if word not in stop_words:\n",
    "            processed_text.append(word)\n",
    "    return processed_text \n",
    "\n",
    "def get_stopwords(POS_lematized_tag):\n",
    "    stopwords = []\n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "    for word in POS_lematized_tag:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "    punctuations = list(str(string.punctuation))\n",
    "    stopwords = stopwords + punctuations\n",
    "\n",
    "    enc = 'utf-8'\n",
    "    with open('stopword_file.csv', 'r', encoding = enc) as f:\n",
    "        reader = csv.reader(f)\n",
    "        keywords = list(reader)\n",
    "    english_stops = [i[0] for i in keywords]\n",
    "    \n",
    "    stopwords = stopwords + punctuations\n",
    "    \n",
    "    return list(set(stopwords))\n",
    "\n",
    "def get_lematized_text(tags):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    adjective_tags = ['JJ','JJR','JJS']\n",
    "    lemmatized_text = []\n",
    "    for word in tags:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "    return lemmatized_text\n",
    "\n",
    "def get_text_from_file(filename):\n",
    "    fp = open(filename, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data\n",
    "\n",
    "def get_text(directory):\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        text += get_text_from_file(\"KeywordDocs/\" + filename)\n",
    "        break\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = get_text(\"KeywordDocs\")  #gets text of all docs in string format\n",
    "    tokens = nltk.word_tokenize(text) #tokenizes the string\n",
    "    POS_tag = nltk.pos_tag(tokens) # adds POS tag to each token\n",
    "    \n",
    "    lematized_text = get_lematized_text(POS_tag) #Removes duplicate gramatical counterparts\n",
    "    POS_lematized_tag = nltk.pos_tag(lematized_text) #adds POS tags to the lematized text\n",
    "    \n",
    "    stop_words = get_stopwords(POS_lematized_tag)\n",
    "\n",
    "    processed_text = get_processed_text(lematized_text,stop_words) # gets the final set of unique words with stopwords removed\n",
    "    unique_text = list(set(processed_text))\n",
    "    print(len(unique_text))\n",
    "    parts = [ unique_text[i:i+int(len(unique_text)/5)] for i in range(0, len(unique_text), int(len(unique_text)/5)) ]\n",
    "    for part in parts:\n",
    "        score = get_score(part)\n",
    "        if not os.path.isfile('Data/textrank.csv'):\n",
    "            with open('Data/textrank.csv', 'w', encoding = 'utf-8') as f:\n",
    "                columnTitleRow = \"Word, Score\\n\"\n",
    "                f.write(columnTitleRow)\n",
    "                for i in range(0,len(part)):\n",
    "                    f.write(\"%s, %d\\n\"%(part[i],score[i]))\n",
    "        else:\n",
    "            with open('Data/frequency.csv', 'a+', encoding = 'utf-8') as f:\n",
    "                for i in range(0,len(part)):\n",
    "                    f.write(\"%s, %d\\n\"%(part[i],score[i]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
